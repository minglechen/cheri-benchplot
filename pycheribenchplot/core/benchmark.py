import io
import re
import uuid
import typing
import json
import asyncio as aio
from pathlib import Path
from enum import Enum
from dataclasses import dataclass, field

import pandas as pd
import asyncssh

from .config import TemplateConfig, TemplateConfigContext
from .instance import InstanceConfig, InstanceInfo, PlatformOptions
from .procstat import ProcstatDataset
from .pidmap import PidMapDataset
from .dataset import (DatasetRegistry, DatasetArtefact, DatasetName, DataSetContainer)
from .analysis import BenchmarkAnalysisRegistry
from .plot import BenchmarkPlot
from .elf import Symbolizer
from .util import new_logger, timing
from ..pmc import PMCStatData


class BenchmarkError(Exception):
    def __init__(self, benchmark, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.benchmark = benchmark
        self.benchmark.logger.error(str(self))

    def __str__(self):
        msg = super().__str__()
        return f"BenchmarkError: {msg} on benchmark instance {self.benchmark.uuid}"


@dataclass
class BenchmarkDataSetConfig(TemplateConfig):
    """
    Define a dataset generated by the benchmark.

    Attributes
    type: identifier of the dataset handler that generates and imports the dataset
    run_options: dataset-specific options to produce the dataset
    """
    type: DatasetName
    run_options: dict[str, any] = field(default_factory=dict)


@dataclass
class BenchmarkRunConfig(TemplateConfig):
    """
    Define a benchmark to run.
    The benchmark configuration will be associated to an instance configuration
    for running the benchmark.
    Note that dataset names generate template replacements for each dataset file.

    Attributes
    name: display name for the benchmark setup
    iterations: the number of iterations of the benchmark to run
    desc: human-readable benchmark setup description
    benchmark_dataset: Configuration for the dataset handler that runs the actual benchmark
    datasets: Additional datasets configuration. Each entry describes a dataset handler that
      is used to generate additional information about the benchmark and process it.
    """
    name: str
    iterations: int
    benchmark_dataset: BenchmarkDataSetConfig
    datasets: dict[str, BenchmarkDataSetConfig]
    desc: str = ""


@dataclass
class BenchmarkRunRecord:
    """
    Record the execution of a benchmark for post-processing. This archives the instance and
    benchmark configurations bound to a specific run of the benchmark, with template parameters
    fully resolved.
    """
    uuid: uuid.UUID
    instance: InstanceConfig
    run: BenchmarkRunConfig


@dataclass
class CommandHistoryEntry:
    """Internal helper to bind commands to PIDs of running tasks"""
    cmd: str
    pid: int = None


class BenchmarkBase(TemplateConfigContext):
    """
    Base class for all the benchmarks
    """
    def __init__(self, manager, config, instance_config, run_id=None):
        super().__init__()
        if run_id:
            self.uuid = run_id
        else:
            self.uuid = uuid.uuid4()
        self.logger = new_logger(f"{config.name}:{instance_config.name}")
        self.manager = manager
        self.instance_manager = manager.instance_manager
        self.manager_config = manager.config
        self.instance_config = instance_config
        self.config = config

        self._bind_early_configs()
        # This is needed before collecting datasets as the output file path uses this.
        self._result_path = self.manager.session_output_path / str(self.uuid)
        self.result_path = self._result_path  # XXX backward compat, remove
        self.datasets = {}
        self._collect_datasets()
        self._bind_configs()

        rootfs_path = self.manager_config.sdk_path / f"rootfs-{self.instance_config.cheri_target}"
        rootfs_path = rootfs_path.expanduser()
        if not rootfs_path.exists() or not rootfs_path.is_dir():
            raise Exception(f"Invalid rootfs path {rootfs_path} for benchmark instance")
        self.rootfs = rootfs_path
        self._result_path.mkdir(parents=True, exist_ok=True)

        self._configure_datasets()

        self._reserved_instance = None  # InstanceInfo of the instance we have allocated
        self._conn = None  # Connection to the CheriBSD instance
        self._command_tasks = []  # Commands being run on the instance
        # Map tasks to CommandHistoryEntry to track the PID of every command we run during the benchmark
        self.command_history = {}
        # Map uuids to benchmarks that have been merged into the current instance (which is the baseline)
        # so that we can look them up if necessary
        self.merged_benchmarks = {}
        # Symbol mapping handler for this benchmark instance
        self.sym_resolver = Symbolizer(self)
        # Current benchmark iteration being run, only valid within the run step.
        self.current_iteration = None
        self.logger.info("Benchmark instance with UUID=%s", self.uuid)

    def _bind_early_configs(self):
        """First pass of configuration template subsitution"""
        self.register_template_subst(uuid=self.uuid,
                                     cheri_target=self.instance_config.cheri_target,
                                     session=self.manager.session)
        self.instance_config = self.instance_config.bind(self)
        self.config = self.config.bind(self)

    def _bind_configs(self):
        """Second pass of configuration template substitution"""
        template_params = {dset.name.replace("-", "_"): dset.output_file() for dset in self.datasets.values()}
        self.register_template_subst(**template_params)
        self.instance_config = self.instance_config.bind(self)
        self.config = self.config.bind(self)

    def _configure_datasets(self):
        """Resolve platform options for the instance configuration and finalize dataset configuration"""
        opts = PlatformOptions()
        for dset in self.datasets.values():
            opts = dset.configure(opts)
        self.instance_config.platform_options = opts

    def _collect_datasets(self):
        """
        Initialize dataset instances from the configuration.
        Note that this must happen before we perform the second pass of configuration template resolution
        XXX-AM: do we really care about template substitution in config anymore?
        """
        # The main dataset for the benchmark
        self.datasets[self.config.benchmark_dataset.type] = self._get_dataset_handler(
            "benchmark", self.config.benchmark_dataset)
        # Implicit auxiliary datasets
        # Procstat dataset should be added in configuration file as it depends on the benchmark
        # self.datasets[DatasetArtefact.PROCSTAT] = self._get_dataset_handler(
        #     "procstat", BenchmarkDataSetConfig(type=DatasetName.PROCSTAT))
        self.datasets[DatasetArtefact.PIDMAP] = self._get_dataset_handler(
            "pidmap", BenchmarkDataSetConfig(type=DatasetName.PIDMAP))
        # Extra datasets configured
        for name, config in self.config.datasets.items():
            assert config.type not in self.datasets, "Duplicate dataset name"
            self.datasets[config.type] = self._get_dataset_handler(name, config)

        # Collect the datasets that are supposed to generate output.
        # These are only used for running the benchmark.
        # If multiple datasets have the same dataset_source_id, assume it to be equivalent and
        # just pick one of them (this is the case for commands that produce multiple datasets).
        self.datasets_gen = {}
        for dset in self.datasets.values():
            if dset.dataset_source_id not in self.datasets_gen:
                self.datasets_gen[dset.dataset_source_id] = dset

        # Summary
        for did, dset in self.datasets.items():
            role = "benchmark" if did == self.config.benchmark_dataset.type else "aux"
            dataset_artefact = dset.dataset_source_id
            generator = self.datasets_gen[dataset_artefact]
            if generator == dset:
                generator_status = f"generator for {dataset_artefact}"
            else:
                generator_status = f"depends on {dataset_artefact}"
            self.logger.debug("Configured %s dataset: %s (%s) %s", role, dset.name, dset.__class__.__name__,
                              generator_status)

    def _get_dataset_handler(self, dset_key: str, config: BenchmarkDataSetConfig):
        """Resolve the parser for the given dataset"""
        ds_name = DatasetName(config.type)
        handler_class = DatasetRegistry.resolve_name(ds_name)
        handler = handler_class(self, dset_key, config)
        return handler

    def _dataset_generators_sorted(self, reverse=False) -> list[DataSetContainer]:
        return sorted(self.datasets_gen.values(), key=lambda ds: ds.dataset_run_order, reverse=reverse)

    def get_output_path(self):
        """
        Get base output path for the current benchmark instance.
        This can be used as a base path or to store data that is not specific to a single iteration
        of the benchmark.
        """
        return self.manager.session_output_path / str(self.uuid)

    def get_iter_output_path(self):
        """
        Return the output directory path for the current benchmark iteration.
        This should be used by all datasets to store the output files.
        """
        assert self.current_iteration is not None, "current_iteration must be set"
        iter_path = self.get_output_path() / str(self.current_iteration)
        iter_path.mkdir(exists_ok=True)
        return iter_path

    def get_dataset(self, name: DatasetName):
        return self.datasets.get(name, None)

    def get_dataset_by_artefact(self, ds_id: DatasetArtefact):
        """
        Lookup a generic dataset by the artefact ID.
        Note that this will fail if there are multiple matches
        """
        match = [dset for dset in self.datasets.values() if dset.dataset_source_id == ds_id]
        if len(match) > 1:
            raise KeyError("Multiple matching dataset for artefact %s", ds_id)
        if len(match):
            return match[0]
        return None

    def _record_benchmark_run(self):
        record = BenchmarkRunRecord(uuid=self.uuid, instance=self.instance_config, run=self.config)
        self.manager.record_benchmark(record)

    def _build_remote_command(self, cmd: str, args: list, env={}):
        """
        asyncss does not return the pid of the process so we need to print it.
        We also use this to work around any restriction on the environment variables
        we are able to set on the guest.
        The first thing that the remote command will print will contain the process PID,
        this is handled by the _cmd_io() loop.
        """
        cmdline = f"{cmd} " + " ".join(map(str, args))
        exports = [f"export {name}={value};" for name, value in env.items()]
        export_line = " ".join(exports)
        sh_cmd = ["sh", "-c", f"'echo $$;{export_line} exec {cmdline}'"]
        return sh_cmd

    def _parse_pid_line(self, ssh_task, line: str) -> int:
        """
        Parse the first line of the output with the process PID number
        """
        try:
            self.logger.debug("Attempt to resolve PID %s", line)
            pid = int(line)
            self.command_history[ssh_task].pid = pid
            self.logger.debug("Bind remote command %s to PID %d", ssh_task.command, pid)
        except ValueError:
            raise BenchmarkError(self, f"Can not determine running process pid for {ssh_task.command}, bailing out.")

    async def _cmd_io(self, proc_task, callback):
        try:
            while proc_task.returncode is None and not proc_task.stdout.at_eof():
                out = await proc_task.stdout.readline()
                if not out:
                    continue
                # Expect to receive the PID of the command in the first output line
                if self.command_history[proc_task].pid is None:
                    try:
                        self._parse_pid_line(proc_task, out)
                    except BenchmarkError as ex:
                        proc_task.terminate()
                        raise ex
                try:
                    if callback:
                        callback(out)
                except aio.CancelledError as ex:
                    raise ex
                except Exception as ex:
                    self.logger.error("Error while processing output for %s: %s", proc_task.command, ex)
                if out:
                    self.logger.debug(out)
        except aio.CancelledError as ex:
            proc_task.terminate()
            raise ex
        finally:
            self.logger.debug("Background task %s done", proc_task.command)

    async def run_bg_cmd(self, command: str, args: list, env={}, iocallback=None, history_command: str = None):
        """
        Run a background command without waiting for termination.

        Arguments:
        command: the command to execute
        args: list of command arguments
        env: any environment variables for the command
        iocallback: a function to be called to inspect the command output. The function
        is called asynchronously.
        history_command: override command name in the command history
        """
        if history_command is None:
            history_command = command
        remote_cmd = self._build_remote_command(command, args, env)
        self.logger.debug("SH exec background: %s", remote_cmd)
        proc_task = await self._conn.create_process(" ".join(remote_cmd))
        self.command_history[proc_task] = CommandHistoryEntry(history_command)
        self._command_tasks.append(aio.create_task(self._cmd_io(proc_task, iocallback)))
        return proc_task

    async def stop_bg_cmd(self, task):
        """
        Work around the unreliability of asyncssh/openssh signal delivery
        """
        try:
            pid = self.command_history[task].pid
            result = await self._conn.run(f"kill -TERM {pid}")
            if result.returncode != 0:
                self.logger.error("Failed to stop remote process %d: %s", pid, task.command)
            else:
                await task.wait()
        except KeyError:
            self.logger.error("Can not stop %s, missing pid", task.command)

    async def run_cmd(self, command: str, args: list, env={}, outfile=None):
        """Run a command and wait for the process to complete"""
        remote_cmd = self._build_remote_command(command, args, env)
        self.logger.debug("SH exec: %s", remote_cmd)
        result = await self._conn.run(" ".join(remote_cmd))
        if result.returncode != 0:
            self.logger.error("Failed to run %s: %s", command, result.stderr)
        else:
            self.logger.debug("%s done: %s", command, result.stdout)
        self.command_history[result] = CommandHistoryEntry(command)
        stdout = io.StringIO(result.stdout)
        # Expect to receive the PID of the command in the first output line
        self._parse_pid_line(result, stdout.readline())
        if outfile:
            outfile.write(stdout.read())
        return result

    async def extract_file(self, guest_src: Path, host_dst: Path):
        """Extract file from instance"""
        src = (self._conn, guest_src)
        await asyncssh.scp(src, host_dst)

    async def import_file(self, host_src: Path, guest_dst: Path):
        """Import file into instance"""
        dst = (self._conn, guest_dst)
        await asyncssh.scp(host_src, dst)

    async def _connect_instance(self, info: InstanceInfo):
        conn = await asyncssh.connect(info.ssh_host,
                                      port=info.ssh_port,
                                      known_hosts=None,
                                      client_keys=[self.manager_config.ssh_key],
                                      username="root",
                                      passphrase="")
        self.logger.debug("Connected to instance")
        return conn

    async def run(self):
        bench_dset = self.get_dataset(self.config.benchmark_dataset.type)
        assert bench_dset, "Missing benchmark dataset"
        self.logger.info("Waiting for instance")
        self._reserved_instance = await self.instance_manager.request_instance(self.uuid, self.instance_config)
        try:
            self._conn = await self._connect_instance(self._reserved_instance)
            pre_generators = self._dataset_generators_sorted()
            post_generators = self._dataset_generators_sorted(reverse=True)

            for dset in pre_generators:
                await dset.run_pre_benchmark()

            for i in range(self.config.iterations):
                self.current_iteration = i
                self.logger.info("Running benchmark iteration %d", i)
                for dset in pre_generators:
                    await dset.run_pre_benchmark_iter()
                # Only run the benchmark step for the given benchmark_dataset
                with timing("Benchmark completed", logger=self.logger):
                    await bench_dset.run_benchmark()
                for dset in post_generators:
                    await dset.run_post_benchmark_iter()

            for dset in post_generators:
                await dset.run_post_benchmark()

            # Record successful run and cleanup any pending background task
            self._record_benchmark_run()
            for t in self._command_tasks:
                t.cancel()
            await aio.gather(*self._command_tasks, return_exceptions=True)
        except Exception as ex:
            self.logger.exception("Benchmark run failed: %s", ex)
            self.manager.failed_benchmarks.append(self)
        finally:
            # Just in case, to avoid accidental reuse
            self.current_iteration = None
            await self.instance_manager.release_instance(self.uuid)

    def _load_kernel_symbols(self):
        kernel = self.rootfs / "boot" / f"kernel.{self.instance_config.kernel}" / "kernel.full"
        if not kernel.exists():
            self.logger.warning("Kernel name not found in kernel.<CONF> directories, using the default kernel")
            kernel = self.rootfs / "kernel" / "kernel.full"
        self.sym_resolver.register_sym_source(0, "kernel.full", kernel, shared=True)

    def register_mapped_binary(self, map_addr: int, path: Path):
        """
        Add a binary to the symbolizer for this benchmark.
        The symbols will be associated with the current benchmark address-space.
        """
        bench_dset = self.get_dataset(self.config.benchmark_dataset.type)
        addrspace = bench_dset.get_addrspace_key()
        self.sym_resolver.register_sym_source(map_addr, addrspace, path)

    def load(self):
        """
        Setup benchmark metadata and load results into datasets from the currently assigned run configuration.
        Note: this runs in the aio loop executor asyncronously
        """
        self._load_kernel_symbols()
        for dset in self.datasets.values():
            dset_file = dset.output_file()
            self.logger.info("Loading %s from %s", dset.name, dset_file)
            dset.load(dset_file)

    def pre_merge(self):
        """
        Perform pre-processing step for all datasets. This may generate derived fields.
        Note: this runs in the aio loop executor asyncronously
        """
        for dset in self.datasets.values():
            self.logger.info("Pre-process %s", dset.name)
            dset.pre_merge()

    def load_and_pre_merge(self):
        """Shortcut to perform both the load and pre-merge steps"""
        self.load()
        self.pre_merge()

    def merge(self, others: list["BenchmarkBase"]):
        """
        Merge datasets from compatible runs into a single dataset.
        Note that this is called only on the baseline benchmark instance
        """
        self.logger.debug("Merge datasets %s onto baseline %s", [str(b) for b in others], self.uuid)
        for dset in self.datasets.values():
            dset.init_merge()
        for bench in others:
            self.merged_benchmarks[bench.uuid] = bench
            for parser_id, dset in bench.datasets.items():
                self.datasets[parser_id].merge(dset)
            for parser_id, dset in bench.datasets.items():
                self.datasets[parser_id].post_merge()

    def aggregate(self):
        """
        Generate dataset aggregates (e.g. mean and quartiles)
        Note that this is called only on the baseline benchmark instance
        """
        self.logger.debug("Aggregate datasets %s", self.config.name)
        for dset in self.datasets.values():
            dset.aggregate()
            dset.post_aggregate()

    def analyse(self):
        """
        Run analysis steps on this benchmark. This includes plotting.
        Currently there is no ordering guarantee among analysis steps.
        Note that this is called only on the baseline benchmark instance
        """
        self.logger.debug("Analise %s", self.config.name)
        analysers = []
        for handler_class in BenchmarkAnalysisRegistry.analysis_steps:
            if handler_class.check_required_datasets(self.datasets.keys()):
                analysers.append(handler_class(self))
        self.logger.debug("Resolved analysys steps %s", [str(a) for a in analysers])
        for handler in analysers:
            handler.process_datasets()

    def __str__(self):
        return f"{self.config.name}({self.uuid})"
