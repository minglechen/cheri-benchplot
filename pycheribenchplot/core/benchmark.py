import io
import re
import uuid
import typing
import json
import asyncio as aio
import traceback
from pathlib import Path
from enum import Enum
from dataclasses import dataclass, field

import pandas as pd
import asyncssh

from .config import TemplateConfig, TemplateConfigContext
from .instance import InstanceConfig, InstanceInfo, PlatformOptions
from .procstat import ProcstatDataset
from .pidmap import PidMapDataset
from .dataset import DatasetRegistry, DatasetID
from .analysis import BenchmarkAnalysisRegistry
from .plot import BenchmarkPlot
from .elf import Symbolizer
from .util import new_logger, timing
from ..pmc import PMCStatData


class BenchmarkError(Exception):
    def __init__(self, benchmark, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.benchmark = benchmark
        self.benchmark.logger.error(str(self))

    def __str__(self):
        msg = super().__str__()
        return f"BenchmarkError: {msg} on benchmark instance {self.benchmark.uuid}"


class BenchmarkType(Enum):
    NETPERF = "netperf"
    TEST = "TEST"  # Reserved for tests

    def __str__(self):
        return self.value


@dataclass
class BenchmarkDataSetConfig(TemplateConfig):
    """
    Define a dataset generated by the benchmark.

    Attributes
    type: identifier of the dataset handler that generates and imports the dataset
    """
    type: DatasetID
    run_options: dict[str, any] = field(default_factory=dict)


@dataclass
class BenchmarkRunConfig(TemplateConfig):
    """
    Define a benchmark to run.
    The benchmark configuration will be associated to an instance configuration
    for running the benchmark.
    Note that dataset names generate template replacements for each dataset file.

    Attributes
    name: display name for the benchmark setup
    desc: human-readable benchmark setup description
    benchmark_dataset: Configuration for the dataset handler that runs the actual benchmark
    datasets: Additional datasets configuration. Each entry describes a dataset handler that
      is used to generate additional information about the benchmark and process it.
    """
    name: str
    type: BenchmarkType
    benchmark_dataset: BenchmarkDataSetConfig
    datasets: dict[str, BenchmarkDataSetConfig]
    desc: str = ""


@dataclass
class BenchmarkRunRecord:
    """
    Record the execution of a benchmark for post-processing. This archives the instance and
    benchmark configurations bound to a specific run of the benchmark, with template parameters
    fully resolved.
    """
    uuid: uuid.UUID
    instance: InstanceConfig
    run: BenchmarkRunConfig


@dataclass
class CommandHistoryEntry:
    """Internal helper to bind commands to PIDs of running tasks"""
    cmd: str
    pid: int = None


class BenchmarkBase(TemplateConfigContext):
    """
    Base class for all the benchmarks
    """
    def __init__(self, manager, config, instance_config, run_id=None):
        super().__init__()
        if run_id:
            self.uuid = run_id
        else:
            self.uuid = uuid.uuid4()
        self.logger = new_logger(f"{config.name}:{instance_config.name}:{self.uuid}")
        self.manager = manager
        self.instance_manager = manager.instance_manager
        self.manager_config = manager.config
        self.instance_config = instance_config
        self.config = config

        self._bind_early_configs()
        # This is needed before collecting datasets as the output file path uses this.
        self.result_path = self.manager.session_output_path / str(self.uuid)
        self.datasets = {}
        self._collect_datasets()
        self._bind_configs()

        rootfs_path = self.manager_config.sdk_path / f"rootfs-{self.instance_config.cheri_target}"
        rootfs_path = rootfs_path.expanduser()
        if not rootfs_path.exists() or not rootfs_path.is_dir():
            raise Exception(f"Invalid rootfs path {rootfs_path} for benchmark instance")
        self.rootfs = rootfs_path
        self.result_path.mkdir(parents=True, exist_ok=True)

        self._configure_datasets()

        self._reserved_instance = None  # InstanceInfo of the instance we have allocated
        self._conn = None  # Connection to the CheriBSD instance
        self._command_tasks = []  # Commands being run on the instance
        # Map tasks to CommandHistoryEntry to track the PID of every command we run during the benchmark
        self.command_history = {}
        # Map uuids to benchmarks that have been merged into the current instance (which is the baseline)
        # so that we can look them up if necessary
        self.merged_benchmarks = {}
        self.sym_resolver = Symbolizer()

    def _bind_early_configs(self):
        """First pass of configuration template subsitution"""
        self.register_template_subst(uuid=self.uuid,
                                     cheri_target=self.instance_config.cheri_target,
                                     session=self.manager.session)
        self.instance_config = self.instance_config.bind(self)
        self.config = self.config.bind(self)

    def _bind_configs(self):
        """Second pass of configuration template substitution"""
        template_params = {dset.name.replace("-", "_"): dset.output_file() for dset in self.datasets.values()}
        self.register_template_subst(**template_params)
        self.instance_config = self.instance_config.bind(self)
        self.config = self.config.bind(self)

    def _configure_datasets(self):
        """Resolve platform options for the instance configuration and finalize dataset configuration"""
        opts = PlatformOptions()
        for dset in self.datasets.values():
            opts = dset.configure(opts)
        self.instance_config.platform_options = opts

    def _collect_datasets(self):
        """
        Initialize dataset instances from the configuration.
        Note that this must happen before we perform the second pass of configuration template resolution
        """
        # The main dataset for the benchmark
        self.datasets[self.config.benchmark_dataset.type] = self._get_dataset_handler(
            "benchmark", self.config.benchmark_dataset)
        # Implicit auxiliary datasets
        self.datasets[DatasetID.PROCSTAT] = self._get_dataset_handler("procstat",
                                                                      BenchmarkDataSetConfig(type=DatasetID.PROCSTAT))
        self.datasets[DatasetID.PIDMAP] = self._get_dataset_handler("pidmap",
                                                                    BenchmarkDataSetConfig(type=DatasetID.PIDMAP))
        # Extra datasets configured
        for name, config in self.config.datasets.items():
            assert config.type not in self.datasets, "Duplicate dataset name"
            self.datasets[config.type] = self._get_dataset_handler(name, config)

        # Collect the datasets that are supposed to generate output.
        # If multiple datasets produce the same output, assume it to be equivalent and
        # just pick one of them (this is the case for commands that produce multiple datasets).
        self.datasets_gen = {dset.output_file(): dset for dset in self.datasets.values()}

        # Summary
        for did, dset in self.datasets.items():
            role = "benchmark" if did == self.config.benchmark_dataset.type else "aux"
            dataset_file = dset.output_file()
            generator = self.datasets_gen[dataset_file]
            if generator == dset:
                generator_status = f"generator for {dataset_file}"
            else:
                generator_status = f"depends on {dataset_file}"
            self.logger.debug("Configured %s dataset: %s %s", role, dset.name, generator_status)

    def _get_dataset_handler(self, dset_key: str, config: BenchmarkDataSetConfig):
        """Resolve the parser for the given dataset"""
        did = DatasetID(config.type)
        handler_class = DatasetRegistry.dataset_types.get(did, None)
        if handler_class is None:
            self.logger.error("No handler for dataset %s", config.name)
            raise Exception("Missing handler")
        handler = handler_class(self, dset_key, config)
        return handler

    def get_dataset(self, parser_id: DatasetID):
        return self.datasets.get(parser_id, None)

    def _record_benchmark_run(self):
        record = BenchmarkRunRecord(uuid=self.uuid, instance=self.instance_config, run=self.config)
        self.manager.record_benchmark(record)

    def _build_remote_command(self, cmd: str, args: list, env={}):
        """
        asyncss does not return the pid of the process so we need to print it.
        We also use this to work around any restriction on the environment variables
        we are able to set on the guest.
        The first thing that the remote command will print will contain the process PID,
        this is handled by the _cmd_io() loop.
        """
        cmdline = f"{cmd} " + " ".join(args)
        exports = [f"export {name}={value};" for name, value in env.items()]
        export_line = " ".join(exports)
        sh_cmd = ["sh", "-c", f"'echo $$;{export_line} exec {cmdline}'"]
        return sh_cmd

    def _parse_pid_line(self, ssh_task, line: str) -> int:
        """
        Parse the first line of the output with the process PID number
        """
        try:
            self.logger.debug("Attempt to resolve PID %s", line)
            pid = int(line)
            self.command_history[ssh_task].pid = pid
            self.logger.debug("Bind remote command %s to PID %d", ssh_task.command, pid)
        except ValueError:
            raise BenchmarkError(self, f"Can not determine running process pid for {ssh_task.command}, bailing out.")

    async def _cmd_io(self, proc_task, callback):
        try:
            while proc_task.returncode is None and not proc_task.stdout.at_eof():
                out = await proc_task.stdout.readline()
                if not out:
                    continue
                # Expect to receive the PID of the command in the first output line
                if self.command_history[proc_task].pid is None:
                    try:
                        self._parse_pid_line(proc_task, out)
                    except BenchmarkError as ex:
                        proc_task.terminate()
                        raise ex
                try:
                    if callback:
                        callback(out)
                except aio.CancelledError as ex:
                    raise ex
                except Exception as ex:
                    self.logger.error("Error while processing output for %s: %s", proc_task.command, ex)
                if out:
                    self.logger.debug(out)
        except aio.CancelledError as ex:
            proc_task.terminate()
            raise ex
        finally:
            self.logger.debug("Background task %s done", proc_task.command)

    async def run_bg_cmd(self, command: str, args: list, env={}, iocallback=None):
        """Run a background command without waiting for termination"""
        remote_cmd = self._build_remote_command(command, args, env)
        self.logger.debug("SH exec background: %s", remote_cmd)
        proc_task = await self._conn.create_process(" ".join(remote_cmd))
        self.command_history[proc_task] = CommandHistoryEntry(command)
        self._command_tasks.append(aio.create_task(self._cmd_io(proc_task, iocallback)))
        return proc_task

    async def stop_bg_cmd(self, task):
        """
        Work around the unreliability of asyncssh/openssh signal delivery
        """
        try:
            pid = self.command_history[task].pid
            result = await self._conn.run(f"kill -TERM {pid}")
            if result.returncode != 0:
                self.logger.error("Failed to stop remote process %d: %s", pid, task.command)
            else:
                await task.wait()
        except KeyError:
            self.logger.error("Can not stop %s, missing pid", task.command)

    async def run_cmd(self, command: str, args: list, env={}, outfile=None):
        """Run a command and wait for the process to complete"""
        remote_cmd = self._build_remote_command(command, args, env)
        self.logger.debug("SH exec: %s", remote_cmd)
        result = await self._conn.run(" ".join(remote_cmd))
        if result.returncode != 0:
            self.logger.error("Failed to run %s: %s", command, result.stderr)
        else:
            self.logger.debug("%s done: %s", command, result.stdout)
        self.command_history[result] = CommandHistoryEntry(command)
        stdout = io.StringIO(result.stdout)
        # Expect to receive the PID of the command in the first output line
        self._parse_pid_line(result, stdout.readline())
        if outfile:
            outfile.write(stdout.read())
        return result

    async def extract_file(self, guest_src: Path, host_dst: Path):
        """Extract file from instance"""
        src = (self._conn, guest_src)
        await asyncssh.scp(src, host_dst)

    async def import_file(self, host_src: Path, guest_dst: Path):
        """Import file into instance"""
        dst = (self._conn, guest_dst)
        await asyncssh.scp(host_src, dst)

    async def _connect_instance(self, info: InstanceInfo):
        conn = await asyncssh.connect(info.ssh_host,
                                      port=info.ssh_port,
                                      known_hosts=None,
                                      client_keys=[self.manager_config.ssh_key],
                                      username="root",
                                      passphrase="")
        self.logger.debug("Connected to instance")
        return conn

    async def run(self):
        bench_dset = self.get_dataset(self.config.benchmark_dataset.type)
        assert bench_dset, "Missing benchmark dataset"
        self.logger.info("Waiting for instance")
        self._reserved_instance = await self.instance_manager.request_instance(self.uuid, self.instance_config)
        try:
            self._conn = await self._connect_instance(self._reserved_instance)
            for dset in self.datasets_gen.values():
                await dset.run_pre_benchmark()
            # Only run the benchmark step for the given benchmark_dataset
            with timing("Benchmark completed", logger=self.logger):
                await bench_dset.run_benchmark()
            for dset in self.datasets_gen.values():
                await dset.run_post_benchmark()
            self._record_benchmark_run()
            # Stop all pending background processes
            for t in self._command_tasks:
                t.cancel()
            await aio.gather(*self._command_tasks, return_exceptions=True)
        except Exception as ex:
            self.logger.exception("Benchmark run failed: %s", ex)
            self.manager.failed_benchmarks.append(self)
        finally:
            await self.instance_manager.release_instance(self.uuid)

    def _load_kernel_symbols(self):
        kernel = self.rootfs / "boot" / f"kernel.{self.instance_config.kernel}" / "kernel.full"
        if not kernel.exists():
            self.logger.warning("Kernel name not found in kernel.<CONF> directories, using the default kernel")
            kernel = self.rootfs / "kernel" / "kernel.full"
        self.sym_resolver.register_sym_source(0, "kernel.full", kernel, shared=True)

    def register_mapped_binary(self, map_addr: int, path: Path):
        """
        Add a binary to the symbolizer for this benchmark.
        The symbols will be associated with the current benchmark address-space.
        """
        bench_dset = self.get_dataset(self.config.benchmark_dataset.type)
        addrspace = bench_dset.get_addrspace_key()
        self.sym_resolver.register_sym_source(map_addr, addrspace, path)

    def load(self):
        """
        Setup benchmark metadata and load results into datasets from the currently assigned run configuration.
        Note: this runs in the aio loop executor asyncronously so beware of concurrency quirks
        """
        self._load_kernel_symbols()
        for dset in self.datasets.values():
            dset_file = dset.output_file()
            self.logger.info("Loading %s from %s", dset.name, dset_file)
            dset.load(dset_file)
        for dset in self.datasets.values():
            dset.pre_merge()

    def merge(self, others: list["BenchmarkBase"]):
        """
        Merge datasets from compatible runs into a single dataset.
        Note that this is called only on the baseline benchmark instance
        """
        self.logger.debug("Merge datasets %s onto baseline %s", [str(b) for b in others], self.uuid)
        for dset in self.datasets.values():
            dset.init_merge()
        for bench in others:
            self.merged_benchmarks[bench.uuid] = bench
            for parser_id, dset in bench.datasets.items():
                self.datasets[parser_id].merge(dset)
            for parser_id, dset in bench.datasets.items():
                self.datasets[parser_id].post_merge()

    def aggregate(self):
        """
        Generate dataset aggregates (e.g. mean and quartiles)
        Note that this is called only on the baseline benchmark instance
        """
        self.logger.debug("Aggregate datasets %s", self.config.name)
        for dset in self.datasets.values():
            dset.aggregate()
            dset.post_aggregate()

    def verify(self):
        """
        Verify the integrity of the aggregate / post-processed data
        """
        self.logger.debug("Verify dataset integrity for %s", self.config.name)

    def plot(self):
        """
        Plot the data from the generated datasets
        Note that this is called only on the baseline benchmark instance
        """
        self.logger.info("Plot datasets")
        plotters = []
        for step_klass in BenchmarkAnalysisRegistry.analysis_steps:
            if issubclass(step_klass, BenchmarkPlot) and step_klass.check_required_datasets(self.datasets.keys()):
                plot = step_klass(self)
                plotters.append(plot)
        self.logger.debug("Resolved plotters %s", plotters)
        for plot in plotters:
            plot.process_datasets()

    def __str__(self):
        return f"{self.config.name}:{self.uuid}"
