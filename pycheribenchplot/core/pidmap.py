import io
import json
import re
import typing
from pathlib import Path

import numpy as np
import pandas as pd

from .dataset import DatasetArtefact, DatasetName, Field
from .json import JSONDataSetContainer


class PidMapDataset(JSONDataSetContainer):
    """
    JSON output generated by the --libxo option of the ps command
    Currently we only support a subset of the output values
    """
    dataset_config_name = DatasetName.PIDMAP
    dataset_source_id = DatasetArtefact.PIDMAP
    fields = [
        Field("pid", dtype=int),
        Field("tid", dtype=int),
        Field.str_field("command"),
        Field.str_field("thread_name")
    ]

    def resolve_user_binaries(self, dataset_id) -> pd.DataFrame:
        df = self.df.xs(dataset_id)
        P_KPROC_MASK = 0x4
        user_commands = (df["flags"] & P_KPROC_MASK) == 0

        def resolve_path(cmd):
            target_path = Path(cmd)
            if target_path.is_absolute():
                return self.benchmark.rootfs / target_path.relative_to("/")
            else:
                # attempt to locate file in /bin /usr/bin /usr/sbin
                dollarpath = [Path("bin"), Path("usr/bin"), Path("usr/sbin")]
                for path in dollarpath:
                    candidate = self.benchmark.rootfs / path / target_path
                    if candidate.exists():
                        return candidate
            return None

        result_df = df.copy()
        result_df["command"] = df["command"].map(resolve_path)
        return result_df[~result_df["command"].isna()]

    def output_file(self):
        return super().output_file().with_suffix(".json")

    def load(self):
        df = self._load_json(self.output_file())
        df["dataset_id"] = self.benchmark.uuid
        # Drop duplicate (pid, command) pairs, under the assumption that the same process will be mapped
        # in the same way upon multiple runs.
        df = df.drop_duplicates(subset=["pid", "command"], keep="first")
        self._append_df(df)

    def load_from_kdump(self, kdump_fd):
        """
        Add a system PID sample to the dataset from the output of kdump
        We inspect the output for fork() syscalls. The dollarpath must point
        to the directory where the forked executables can be found.
        """
        cols = ["pid", "command", "op", "args"]
        rows = []
        for line in kdump_fd:
            rows.append(re.split("\s+", line.lstrip(), len(cols) - 1))
        kdump_df = pd.DataFrame.from_records(rows, columns=cols)
        forks = (kdump_df["op"] == "RET") & (kdump_df["args"].str.startswith("fork"))
        fork_df = kdump_df[forks].reset_index(drop=True)
        # fork return is in the form "fork <pid>/<hex-pid>"
        fork_df["child_pid"] = fork_df["args"].map(lambda arg: arg.split(" ")[1].split("/")[0])
        pid_df = fork_df[["child_pid", "command"]].rename(columns={"child_pid": "pid"})
        pid_df["tid"] = -1
        pid_df["thread_name"] = ""
        pid_df["dataset_id"] = self.benchmark.uuid
        # Assume that the pids do not overlap
        self._append_df(pid_df)

    def fixup_missing_tid(self, tid_mapping: pd.DataFrame) -> pd.DataFrame:
        """
        Return a copy of the main dataframe replacing the missing TID values
        using the given mapping dataframe.
        """
        assert "pid" in tid_mapping.columns
        assert "tid" in tid_mapping.columns
        tid_mapping = tid_mapping.add_suffix("_fixup")

        missing_tid = self.df["tid"] == -1
        matched_tid = self.df[~missing_tid]
        guess_tid = self.df[missing_tid].merge(tid_mapping, how="left", left_on="pid", right_on="pid_fixup")
        guess_tid["tid"] = guess_tid["tid_fixup"].fillna(-1).astype(int)
        fixup_df = pd.concat((matched_tid, guess_tid[self.input_non_index_columns()]))
        return fixup_df

    def _merge_procstat_entry(self, pidmap: dict, entry: dict):
        pid = entry["process_id"]
        if str(pid) in pidmap:
            map_entry = pidmap[pid]
            # Currently, do not support PID/TID reuse
            if map_entry["command"] != entry["command"]:
                self.logger.error("System PID snapshot repeated PID conflict %d: %s != %s", entry["process_id"],
                                  entry["command"], map_entry["command"])
                return
            for tid, thr_entry in entry["threads"].items():
                if tid in map_entry["threads"]:
                    snap_thr_entry = map_entry["threads"][tid]
                    if snap_thr_entry["thread_name"] != thr_entry["thread_name"]:
                        self.logger.error("System PID snapshot repeated TID conflict for %s %d: %s != %s",
                                          entry["command"], thr_entry["thread_id"], thr_entry["thread_name"],
                                          snap_thr_entry["thread_name"])
                        continue
                else:
                    map_entry["threads"][tid] = thr_entry
        else:
            pidmap[pid] = entry

    def sample_system_pids(self, script):
        """
        Add a system PID sample to the output data.
        This is exposed to other datasets so that snapshots can be taken at interesting
        times.
        """
        self.logger.debug("Generate system PIDs sampling command")
        script.gen_cmd("procstat", ["-a", "-t", "--libxo", "json"], outfile=self.output_file())

    async def after_extract_results(self, script, instance):
        """
        Merge all pid information sources and normalize the procstat output in a format easily loadable by pandas.
        """
        await super().after_extract_results(script, instance)

        pidmap = {}
        # Read in the output file
        with open(self.output_file(), "r") as fd:
            raw_data = json.load(fd)
        # The procstat output has useless dictionary levels, drop them
        try:
            proc_map = raw_data["procstat"]["threads"]
        except KeyError:
            self.logger.error("Malformed procstat output missing procstat.threads %s", raw_data)
            raise
        for pid, info in proc_map.items():
            self._merge_procstat_entry(pidmap, info)

        # Grab the command history and merge it as well
        cmd_pids = pd.read_csv(script.command_history_path(), dtype=int, index_col=False, names=["pid"])
        cmd_pids["command"] = script.get_commands_with_pid()
        for _, row in cmd_pids.iterrows():
            self.logger.debug("Recording command PID %s => %s", row["pid"], row["command"])
            entry = {
                "process_id": row["pid"],
                "command": str(row["command"]),
                # We do not have thread info for these, should really use a different method
                "threads": {}
            }
            self._merge_procstat_entry(pidmap, entry)

        # Now we have the final pidmap, normalize the json records to use lists instead of dicts
        # as this will make it easier to load into a dataframe
        for entry in pidmap.values():
            entry["threads"] = list(entry["threads"].values())
        pidmap_records = list(pidmap.values())
        # generate a normalized dataframe from hierarchical data
        pid_info = pd.json_normalize(pidmap_records)
        tid_info = pd.json_normalize(pidmap_records, "threads", ["process_id"])
        df = pid_info.merge(tid_info, how="left", left_on="process_id", right_on="process_id")
        df.rename(columns={"process_id": "pid", "thread_id": "tid"}, inplace=True)
        df["tid"].fillna(-1, inplace=True)

        with open(self.output_file(), "w+") as fd:
            fd.truncate(0)
            df.to_json(fd)

    def gen_post_benchmark(self, script):
        """
        Post-benchmark hook to extract PID mappings.
        Note: we also use the command history from the benchmark runner to resolve any
        extra processes we have been running and that have since terminated.
        """
        super().gen_post_benchmark(script)
        self.sample_system_pids(script)
