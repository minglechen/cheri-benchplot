import json
from pathlib import Path

import pandas as pd
import numpy as np

from ..core.dataset import (DatasetID, DataField, Field, IndexField, align_multi_index_levels)
from ..core.json import JSONDataSetContainer


class VMStatDataset(JSONDataSetContainer):
    """
    JSON output generated by the --libxo option of the vmstat command
    Note that we expect two files to exist with the suffix ".pre" and ".post"
    containing vmstat snapshots before and after the benchmark
    """
    def _get_vmstat_records(self, data):
        raise NotImplementedError("Must be defined by subclasses")

    def _vmstat_delta(self, pre_df, post_df):
        raise NotImplementedError("Must be defined by subclasses")

    def load(self, path: Path):
        pre = open(path.with_suffix(".pre"), "r")
        post = open(path.with_suffix(".post"), "r")
        try:
            pre_data = json.load(pre)
            post_data = json.load(post)
            pre_df = pd.DataFrame.from_records(self._get_vmstat_records(pre_data))
            post_df = pd.DataFrame.from_records(self._get_vmstat_records(post_data))
            df = self._vmstat_delta(pre_df, post_df)
            df["__dataset_id"] = self.benchmark.uuid
            df = df.astype(self._get_column_dtypes())
            self._internalize_json(df)
        finally:
            pre.close()
            post.close()

    def aggregate(self):
        super().aggregate()
        # We just sum if there are repeated index entries
        self.agg_df = self.merged_df.groupby(self.merged_df.index.names).sum()

    def post_aggregate(self):
        super().post_aggregate()
        new_df = align_multi_index_levels(self.agg_df, self._get_align_levels(), fill_value=0)
        # Compute delta for each metric
        baseline = new_df.xs(self.benchmark.uuid, level="__dataset_id")
        datasets = new_df.index.get_level_values("__dataset_id").unique()
        base_cols = self.data_columns()
        delta_cols = [f"delta_{col}" for col in base_cols]
        norm_cols = [f"norm_{col}" for col in delta_cols]
        new_df[delta_cols] = 0
        new_df[norm_cols] = 0
        for ds_id in datasets:
            other = new_df.xs(ds_id, level="__dataset_id")
            delta = other[base_cols].subtract(baseline[base_cols])
            norm_delta = delta[base_cols].divide(baseline[base_cols])
            new_df.loc[ds_id, delta_cols] = delta[base_cols].values
            new_df.loc[ds_id, norm_cols] = norm_delta[base_cols].values
        self.agg_df = new_df

    def output_file(self):
        """
        The output file is shared by all vmstat datasets. This will have the side-effect of having only
        one of the vmstat datasets running the pre/post benchmark steps.
        """
        return self.benchmark.result_path / f"vmstat-{self.benchmark.uuid}.json"

    async def run_pre_benchmark(self):
        """
        Run a vmstat snapshot before the benchmark runs.
        """
        self.logger.info("Pre-benchmark vmstat snapshot")
        pre_output = self.output_file().with_suffix(".pre")
        with open(pre_output, "w+") as vmstat_out:
            await self.benchmark.run_cmd("vmstat", ["--libxo", "json", "-H", "-i", "-m", "-o", "-P", "-z"],
                                         outfile=vmstat_out)

    async def run_post_benchmark(self):
        """
        Run a vmstat snapshot after the benchmark runs.
        """
        self.logger.info("Post-benchmark vmstat snapshot")
        post_output = self.output_file().with_suffix(".post")
        with open(post_output, "w+") as vmstat_out:
            await self.benchmark.run_cmd("vmstat", ["--libxo", "json", "-H", "-i", "-m", "-o", "-P", "-z"],
                                         outfile=vmstat_out)


class VMStatKMalloc(VMStatDataset):
    dataset_id = DatasetID.VMSTAT_MALLOC
    fields = [
        IndexField("type", dtype=str),
        DataField("in-use", dtype=int),
        DataField("memory-use", dtype=int),
        DataField("reservation-use", dtype=int),
        DataField("requests", dtype=int),
        DataField("large-malloc", dtype=int),
        Field("size", dtype=object),
    ]

    def raw_fields(self, include_derived=False):
        return VMStatKMalloc.fields

    def _get_align_levels(self):
        return ["type"]

    def _get_vmstat_records(self, data):
        return data["malloc-statistics"]["memory"]

    def _vmstat_delta(self, pre_df, post_df):
        pre_df = pre_df.set_index("type")
        post_df = post_df.set_index("type")
        pre_df["size"] = pre_df["size"].map(lambda v: set(v))
        post_df["size"] = post_df["size"].map(lambda v: set(v))
        return post_df.subtract(pre_df).reset_index()


class VMStatUMA(VMStatDataset):
    dataset_id = DatasetID.VMSTAT_UMA
    fields = [
        IndexField("name", dtype=str),
        DataField("size", dtype=int),
        DataField("limit", dtype=int),
        DataField("used", dtype=int),
        DataField("free", dtype=int),
        DataField("requests", dtype=int),
        DataField("fail", dtype=int),
        DataField("sleep", dtype=int),
        DataField("xdomain", dtype=int),
    ]

    def raw_fields(self, include_derived=False):
        return VMStatUMA.fields

    def _get_align_levels(self):
        return ["name"]

    def _get_vmstat_records(self, data):
        return data["memory-zone-statistics"]["zone"]

    def _vmstat_delta(self, pre_df, post_df):
        pre_df = pre_df.set_index(["name"])
        post_df = post_df.set_index(["name"])
        # The size field is not supposed to change from pre to post
        # so we use the initial size for each bucket
        delta = post_df.subtract(pre_df)
        delta["size"] = pre_df["size"]
        return delta.reset_index()
