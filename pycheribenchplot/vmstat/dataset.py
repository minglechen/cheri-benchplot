import json
from pathlib import Path

import pandas as pd
import numpy as np

from ..core.dataset import (DatasetID, DataField, Field, IndexField, align_multi_index_levels)
from ..core.json import JSONDataSetContainer


class VMStatDataset(JSONDataSetContainer):
    """
    JSON output generated by the --libxo option of the vmstat command
    Note that we expect two files to exist with the suffix ".pre" and ".post"
    containing vmstat snapshots before and after the benchmark
    """
    def _get_vmstat_records(self, data):
        raise NotImplementedError("Must be defined by subclasses")

    def _vmstat_delta(self, pre_df, post_df):
        raise NotImplementedError("Must be defined by subclasses")

    def load(self, path: Path):
        pre = open(path.with_suffix(".pre"), "r")
        post = open(path.with_suffix(".post"), "r")
        try:
            pre_data = json.load(pre)
            post_data = json.load(post)
            pre_df = pd.DataFrame.from_records(self._get_vmstat_records(pre_data))
            post_df = pd.DataFrame.from_records(self._get_vmstat_records(post_data))
            df = self._vmstat_delta(pre_df, post_df)
            df["__dataset_id"] = self.benchmark.uuid
            df = df.astype(self._get_column_dtypes())
            self._internalize_json(df)
        finally:
            pre.close()
            post.close()


class VMStatKMalloc(VMStatDataset):
    dataset_id = DatasetID.VMSTAT_MALLOC
    fields = [
        IndexField("type", dtype=str),
        DataField("in-use", dtype=int),
        DataField("memory-use", dtype=int),
        DataField("reservation-use", dtype=int),
        DataField("requests", dtype=int),
        DataField("large-malloc", dtype=int),
        Field("size", dtype=object),
    ]

    def raw_fields(self, include_derived=False):
        return VMStatKMalloc.fields

    def _get_vmstat_records(self, data):
        return data["malloc-statistics"]["memory"]

    def _vmstat_delta(self, pre_df, post_df):
        pre_df = pre_df.set_index("type")
        post_df = post_df.set_index("type")
        pre_df["size"] = pre_df["size"].map(lambda v: set(v))
        post_df["size"] = post_df["size"].map(lambda v: set(v))
        return post_df.subtract(pre_df).reset_index()

    def post_aggregate(self):
        super().post_aggregate()
        new_df = align_multi_index_levels(self.agg_df, ["type"], fill_value=0)
        # Compute delta for each metric
        baseline = new_df.xs(self.benchmark.uuid, level="__dataset_id")
        datasets = new_df.index.get_level_values("__dataset_id").unique()
        base_cols = self.data_columns()
        delta_cols = [f"delta_{col}" for col in base_cols]
        norm_cols = [f"norm_{col}" for col in delta_cols]
        new_df[delta_cols] = 0
        new_df[norm_cols] = 0
        for ds_id in datasets:
            other = new_df.xs(ds_id, level="__dataset_id")
            delta = other[base_cols].subtract(baseline[base_cols])
            norm_delta = delta[base_cols].divide(baseline[base_cols])
            new_df.loc[ds_id, delta_cols] = delta[base_cols].values
            new_df.loc[ds_id, norm_cols] = norm_delta[base_cols].values
        self.agg_df = new_df
